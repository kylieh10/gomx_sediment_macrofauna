---
title: "Wrangling the Data"
editor: visual
---

# Using R to get the data from ScienceBase

To retrieve and use the data from ScienceBase, R packages `dplyr`, `sbtools`, `stringr`, and `worrms` will need to be loaded. After loading these packages, we will read the data in from ScienceBase.

The ScienceBase dataset ID can be found at the end of the ScienceBase link for the dataset. If the full link to the data is "https://www.sciencebase.gov/catalog/item/5a709594e4b0a9a2e9d88e4e", then the identifier is `5a709594e4b0a9a2e9d88e4e`.

Using the ScienceBase ID, we will get information about the data files using `item_list_files(sb_id = sb_id)` and assign it to an object `sb_filenames`.

From the object `sb_filenames`, we will pull the column `url`. This column contains the url needed to download the data file from ScienceBase. Rather than download a local copy of the files, we will read it directly into the memory of our computer with `readr:: read_csv(file = sb_filenames$url[n])`, with n being the row number of the file we are reading. Now we have dataframes to work with!

```{r}
#| warning: false
#| message: false

library(dplyr)
library(sbtools)
library(stringr)
library(worrms)
library(tidyr)

sb_id <- '5a709594e4b0a9a2e9d88e4e'

sb_filenames <- item_list_files(sb_id = sb_id)

BTA <-readr::read_csv(file = sb_filenames$url[1])
Infauna <- readr::read_csv(file = sb_filenames$url[2])
SedChem <- readr::read_csv(file = sb_filenames$url[3])
```

# Using R to transform the data

After loading the data, we will have to transform the data to align with DarwinCore standard formats and terms.

## Renaming Columns

Some column names may directly correlate with the definitions of some DwC terms (a 1:1 relationship), meaning we only have to rename them. To preserve the original data, we will create a new table from `Infauna` to do the manipulations. Then, we will use `rename(newName = oldName)` to assign new column names.

```{r}
Infauna_StationCore <- Infauna %>%
  
  rename(
    locationRemarks = Location,
    materialEntityID = CoreID,
    locationID = Station,
    decimalLatitude = Latitude,
    decimalLongitude = Longitude
  )
```

## Creating New Columns

### Mutating

Some renaming might be slightly more complex, requiring manipulation of a column's format or content to fit the DwC standard. For these tasks, we will use the `mutate` function.

In this case, the column may be in the wrong format, like `DateCollected` which needs to be adjusted before being assigned to `eventDate`. Others, like `eventDate` or `samplingProtocol`, are concatenations of other columns (many:1 relationships), which can be combined with `paste`.

```{r}
Infauna_StationCore <- Infauna_StationCore %>%
  mutate(
    geodeticDatum = "WGS84",
    eventDate = DateCollected %>% 
      as.Date("%m/%d/%Y"),
    eventID = paste(Site, eventDate %>% as.character(), locationID, materialEntityID,
                    sep = "_") %>% stringr::str_remove_all(pattern = "-"),
    minimumDepthInMeters = Depth,
    maximumDepthInMeters = Depth,
    locality = paste("BOEM Lease Block", Site),
    higherGeography = paste("Gulf of Mexico",
                            paste("BOEM Lease Block", 
                                  Site), sep = " | "),
    samplingProtocol = paste(Gear, CoreDiameter, sep = "_")
  )
```

### Creating new metadata

## Reconfiguring Tables

For some extensions, like the `extendedMeasurementOrFact` extension, imported data may have different configurations than required by DwC, requiring reconfiguration.

### Pivoting

After renaming and mutating column names, we may have to pivot the table from wide to long format. For `extendedMeasurementOrFact` tables, all columns in wide format need to be pivoted into long format.

Example of Wide Format

```{r}
#| label: tbl-tidyformat wide
#| echo: false
#| warning: false


library(dplyr)
library(flextable)
#library(gt)

example_df <- data.frame(species = c("Tilia americana",
                                     "Pinus strobus"),
                         site_01 = sample(0:5, size = 2),
                         site_02 = sample(0:5, size = 2),
                         site_03 = sample(0:5, size = 2))

long_example <- tidyr::pivot_longer(data = example_df, cols = site_01:site_03, names_to = "site", values_to = "count")

# flextable is awesome, but captions aren't working yet: <https://github.com/quarto-dev/quarto-cli/issues/1556
# for now, just add print lines and maybe switch back in the future

  flextable(example_df) %>%
  set_caption(caption = "Wide Format") %>% 
  fontsize(size = 9) %>% 
  autofit() %>% 
  theme_zebra()

```

Example of Long Format

```{r}
#| label: tbl-tidyformat long
#| echo: false
#| warning: false


library(dplyr)
library(flextable)
#library(gt)

example_df <- data.frame(species = c("Tilia americana",
                                     "Pinus strobus"),
                         site_01 = sample(0:5, size = 2),
                         site_02 = sample(0:5, size = 2),
                         site_03 = sample(0:5, size = 2))

long_example <- tidyr::pivot_longer(data = example_df, cols = site_01:site_03, names_to = "site", values_to = "count")

# flextable is awesome, but captions aren't working yet: <https://github.com/quarto-dev/quarto-cli/issues/1556
# for now, just add print lines and maybe switch back in the future

flextable(long_example) %>%
  set_caption(caption = "Long Format") %>%
  fontsize(size = 9) %>% 
  autofit() %>%  theme_zebra()

# example_df %>% 
#   gt(caption = "Wide Format")
# 
# long_example %>% 
#   gt(caption = "Long Format")

```

This is done using the `pivot_longer` function. We will specify what columns to include in the pivot with `cols = c(columnsToBeIncluded)`. All column names included in the `cols` function will now be under the new column `measurementType`, named using `names_to` and the old columns' values under the new column `measurementValue`, named with `values_to`.

We use pivot to wrangle the eMoF table, and the pertinent chunk of code looks something like this:

```{r}
#| eval: false
pivot_longer(
    cols = c("COREWDTH", "MINCDIST", "MAXCDIST", "ADEPZZ01",
             "PRPCL064", "PRPCL088", "proportionGravel(>2000um)"
             ),
    names_to = "measurementType",
    values_to = "measurementValue",
    values_drop_na = TRUE
    )
```

Here is the full code chunk where we first do a variety of mutates prior to the pivot:

```{r}
SedChem <- SedChem %>%
  mutate(
    SampleID = CoreID
  )

Infauna_emof <- Infauna %>%
  bind_rows(SedChem) %>%

  rename(
    materialEntityID = SampleID,
    locationID = Station
  ) %>%

  mutate(
    eventDate = DateCollected %>%
      as.Date("%m/%d/%Y"),
    eventID = paste(Site, eventDate %>% as.character(), materialEntityID, sep = "_") %>%
      stringr::str_remove_all(pattern = "-"),
    MAXCDIST = str_split_i(Fraction, pattern = "-", i = 2) %>% readr::parse_number() %>%
      as.character(),
    MINCDIST = str_split_i(Fraction, pattern = "-", i = 1) %>% readr::parse_number() %>%
      as.character(),
    "PRPCL064" = as.character(Sand),
    "PRPCL088" = as.character(Mud),
    "proportionGravel(>2000um)" = as.character(Gravel),
    ADEPZZ01 = as.character(Depth),
    COREWDTH = as.character(CoreDiameter)
  ) %>%
pivot_longer(
    cols = c("COREWDTH", "MINCDIST", "MAXCDIST", "ADEPZZ01",
             "PRPCL064", "PRPCL088", "proportionGravel(>2000um)"
             ),
    names_to = "measurementType",
    values_to = "measurementValue",
    values_drop_na = TRUE
    )
```

### Joins

Joins are useful if data from multiple tables need to be included in the final table, like when we make a new table with WoRMS data. `left_join` will join table `x` to table `y` by values in the specified columns, so this function will join the tables `Infauna_Occurrence` and `uniqueAphiaSelectColumns` by matching `AphiaID` from both.

After combining these tables and mutating our final iteration of `scientificNameID`, we have finished our `occurrence` table!

```{r}
#| eval: false
# in this case, TSN was provided because it was listed in the original dataset, however it is not required to have both AphiaID and TSN
Occurrence_Ext <- left_join(Infauna_Occurrence, uniqueAphiaSelectColumns, by = c("AphiaID" = "AphiaID")) %>% 
  mutate(
    TSN = paste("urn:lsid:itis.gov:itis_tsn:", TSN),
    scientificNameID = paste(scientificNameID, TSN, sep = ", ")
  )

```

#Aligning to a Taxonomic Database

Occurrence tables require some taxonomic information that may not be provided in your data. In these cases, we will need to call taxonomic information from WoRMS using the `worrms` package. In addition to the required `scientificName` and `scientificNameID` fields, it is valuable to provide other information if it is available, like `taxonRank`, which is the lowest identifiable taxon of an occurrence, and corresponding parent taxa.

We will assign `AphiaID` from the occurrence table to a new variable `myAphiaID` which we will use to call the corresponding WoRMS data. Adding `lapply` to circumvent limits on the number of inputs, we then use `wm_record(id = x)`, where `x` is `myAphiaID`. From the new table, we just pull the columns that we need using `select`.

```{r}

myAphiaID <- Infauna$AphiaID %>% na.omit() %>% unique()

myAphiaID <- lapply(myAphiaID, function(x) wm_record(id = x)) %>% 
  data.table::rbindlist()

uniqueAphiaSelectColumns <- select(.data = myAphiaID,
scientificname, rank, kingdom, phylum, class, order, family, genus, lsid, AphiaID
) %>%
  rename(
    scientficName = scientificname,
    taxonRank = rank,
    scientificNameID = lsid
  )
```

#Outputting Data

Once our data has been cleaned up, mapped, and is ready to be uploaded to the IPT, we will have to output the data as a csv file. To do this, first use the `select` function to choose what columns to include in the final table. For example, the event table, it will look something like this:

```{r}
#| eval: false
Infauna_Event <- bind_rows(Infauna_StationCore, Infauna_Sample) %>% 
  select(
    eventID,
    parentEventID,
    eventDate,
    locationID,
    decimalLatitude,
    decimalLongitude,
    higherGeography,
    locality,
    geodeticDatum,
    minimumDepthInMeters,
    maximumDepthInMeters,
    samplingProtocol,
    locationRemarks,
    minimumDistanceAboveSurfaceInMeters,
    maximumDistancesAboveSurfaceInMeters,
    materialEntityID
  ) %>% 
  distinct()
# since there were multiple occurrences listed for events in the original dataset, here, we used distinct to make sure only unique events are included
```

Then to export as a `csv` file, we will use `write.csv`. In in this case, we named the new file `gomx_sediment_macrofauna_event_(date of export).csv`. If you are planning to publish your data, it is important to make sure `na = ""`, so that valueless columns will be left blank, and that `fileEncoding = "UTF-8"`, which is what GBIF and OBIS use.

```{r}
#| eval: false

Infauna_Event %>% 
  write.csv(
    paste0("data/gomx_sediment_macrofauna_event_", Sys.Date(), ".csv"),
    na = "",
    fileEncoding = "UTF-8", 
    row.names = FALSE
  )

#make sure you don't pipe write.csv from the previous chunk

```

Now we have a file we can upload to the IPT.
