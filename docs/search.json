[
  {
    "objectID": "wrangling_data_page.html",
    "href": "wrangling_data_page.html",
    "title": "Wrangling the Data",
    "section": "",
    "text": "To retrieve and use the data from ScienceBase, R packages dplyr, sbtools, stringr, and worrms will need to be loaded. After loading these packages, we will read the data in from ScienceBase.\nThe ScienceBase dataset ID can be found at the end of the ScienceBase link for the dataset. If the full link to the data is “https://www.sciencebase.gov/catalog/item/5a709594e4b0a9a2e9d88e4e”, then the identifier is 5a709594e4b0a9a2e9d88e4e.\nUsing the ScienceBase ID, we will get information about the data files using item_list_files(sb_id = sb_id) and assign it to an object sb_filenames.\nFrom the object sb_filenames, we will pull the column url. This column contains the url needed to download the data file from ScienceBase. Rather than download a local copy of the files, we will read it directly into the memory of our computer with readr:: read_csv(file = sb_filenames$url[n]), with n being the row number of the file we are reading. Now we have dataframes to work with!\n\nlibrary(dplyr)\nlibrary(sbtools)\nlibrary(stringr)\nlibrary(worrms)\nlibrary(tidyr)\n\nsb_id &lt;- '5a709594e4b0a9a2e9d88e4e'\n\nsb_filenames &lt;- item_list_files(sb_id = sb_id)\n\nBTA &lt;-readr::read_csv(file = sb_filenames$url[1])\nInfauna &lt;- readr::read_csv(file = sb_filenames$url[2])\nSedChem &lt;- readr::read_csv(file = sb_filenames$url[3])",
    "crumbs": [
      "Wrangling the Data"
    ]
  },
  {
    "objectID": "wrangling_data_page.html#renaming-columns",
    "href": "wrangling_data_page.html#renaming-columns",
    "title": "Wrangling the Data",
    "section": "Renaming Columns",
    "text": "Renaming Columns\nSome column names may directly correlate with the definitions of some DwC terms (a 1:1 relationship), meaning we only have to rename them. To preserve the original data, we will create a new table from Infauna to do the manipulations. Then, we will use rename(newName = oldName) to assign new column names.\n\nInfauna_StationCore &lt;- Infauna %&gt;%\n  \n  rename(\n    locationRemarks = Location,\n    materialEntityID = CoreID,\n    locationID = Station,\n    decimalLatitude = Latitude,\n    decimalLongitude = Longitude\n  )",
    "crumbs": [
      "Wrangling the Data"
    ]
  },
  {
    "objectID": "wrangling_data_page.html#creating-new-columns",
    "href": "wrangling_data_page.html#creating-new-columns",
    "title": "Wrangling the Data",
    "section": "Creating New Columns",
    "text": "Creating New Columns\n\nMutating\nSome renaming might be slightly more complex, requiring manipulation of a column’s format or content to fit the DwC standard. For these tasks, we will use the mutate function.\nIn this case, the column may be in the wrong format, like DateCollected which needs to be adjusted before being assigned to eventDate. Others, like eventDate or samplingProtocol, are concatenations of other columns (many:1 relationships), which can be combined with paste.\n\nInfauna_StationCore &lt;- Infauna_StationCore %&gt;%\n  mutate(\n    geodeticDatum = \"WGS84\",\n    eventDate = DateCollected %&gt;% \n      as.Date(\"%m/%d/%Y\"),\n    eventID = paste(Site, eventDate %&gt;% as.character(), locationID, materialEntityID,\n                    sep = \"_\") %&gt;% stringr::str_remove_all(pattern = \"-\"),\n    minimumDepthInMeters = Depth,\n    maximumDepthInMeters = Depth,\n    locality = paste(\"BOEM Lease Block\", Site),\n    higherGeography = paste(\"Gulf of Mexico\",\n                            paste(\"BOEM Lease Block\", \n                                  Site), sep = \" | \"),\n    samplingProtocol = paste(Gear, CoreDiameter, sep = \"_\")\n  )\n\n\n\nCreating new metadata",
    "crumbs": [
      "Wrangling the Data"
    ]
  },
  {
    "objectID": "wrangling_data_page.html#reconfiguring-tables",
    "href": "wrangling_data_page.html#reconfiguring-tables",
    "title": "Wrangling the Data",
    "section": "Reconfiguring Tables",
    "text": "Reconfiguring Tables\nFor some extensions, like the extendedMeasurementOrFact extension, imported data may have different configurations than required by DwC, requiring reconfiguration.\n\nPivoting\nAfter renaming and mutating column names, we may have to pivot the table from wide to long format. For extendedMeasurementOrFact tables, all columns in wide format need to be pivoted into long format.\nExample of Wide Format\n::: {#tbl-tidyformat wide .cell} ::: {.cell-output-display}\nspeciessite_01site_02site_03Tilia americana312Pinus strobus531\n::: :::\nExample of Long Format\n::: {#tbl-tidyformat long .cell} ::: {.cell-output-display}\nspeciessitecountTilia americanasite_011Tilia americanasite_022Tilia americanasite_033Pinus strobussite_013Pinus strobussite_024Pinus strobussite_034\n::: :::\nThis is done using the pivot_longer function. We will specify what columns to include in the pivot with cols = c(columnsToBeIncluded). All column names included in the cols function will now be under the new column measurementType, named using names_to and the old columns’ values under the new column measurementValue, named with values_to.\nWe use pivot to wrangle the eMoF table, and the pertinent chunk of code looks something like this:\n\npivot_longer(\n    cols = c(\"COREWDTH\", \"MINCDIST\", \"MAXCDIST\", \"ADEPZZ01\",\n             \"PRPCL064\", \"PRPCL088\", \"proportionGravel(&gt;2000um)\"\n             ),\n    names_to = \"measurementType\",\n    values_to = \"measurementValue\",\n    values_drop_na = TRUE\n    )\n\nHere is the full code chunk where we first do a variety of mutates prior to the pivot:\n\nSedChem &lt;- SedChem %&gt;%\n  mutate(\n    SampleID = CoreID\n  )\n\nInfauna_emof &lt;- Infauna %&gt;%\n  bind_rows(SedChem) %&gt;%\n\n  rename(\n    materialEntityID = SampleID,\n    locationID = Station\n  ) %&gt;%\n\n  mutate(\n    eventDate = DateCollected %&gt;%\n      as.Date(\"%m/%d/%Y\"),\n    eventID = paste(Site, eventDate %&gt;% as.character(), materialEntityID, sep = \"_\") %&gt;%\n      stringr::str_remove_all(pattern = \"-\"),\n    MAXCDIST = str_split_i(Fraction, pattern = \"-\", i = 2) %&gt;% readr::parse_number() %&gt;%\n      as.character(),\n    MINCDIST = str_split_i(Fraction, pattern = \"-\", i = 1) %&gt;% readr::parse_number() %&gt;%\n      as.character(),\n    \"PRPCL064\" = as.character(Sand),\n    \"PRPCL088\" = as.character(Mud),\n    \"proportionGravel(&gt;2000um)\" = as.character(Gravel),\n    ADEPZZ01 = as.character(Depth),\n    COREWDTH = as.character(CoreDiameter)\n  ) %&gt;%\npivot_longer(\n    cols = c(\"COREWDTH\", \"MINCDIST\", \"MAXCDIST\", \"ADEPZZ01\",\n             \"PRPCL064\", \"PRPCL088\", \"proportionGravel(&gt;2000um)\"\n             ),\n    names_to = \"measurementType\",\n    values_to = \"measurementValue\",\n    values_drop_na = TRUE\n    )\n\n\n\nJoins\nJoins are useful if data from multiple tables need to be included in the final table, like when we make a new table with WoRMS data. left_join will join table x to table y by values in the specified columns, so this function will join the tables Infauna_Occurrence and uniqueAphiaSelectColumns by matching AphiaID from both.\nAfter combining these tables and mutating our final iteration of scientificNameID, we have finished our occurrence table!\n\n# in this case, TSN was provided because it was listed in the original dataset, however it is not required to have both AphiaID and TSN\nOccurrence_Ext &lt;- left_join(Infauna_Occurrence, uniqueAphiaSelectColumns, by = c(\"AphiaID\" = \"AphiaID\")) %&gt;% \n  mutate(\n    TSN = paste(\"urn:lsid:itis.gov:itis_tsn:\", TSN),\n    scientificNameID = paste(scientificNameID, TSN, sep = \", \")\n  )\n\n#Aligning to a Taxonomic Database\nOccurrence tables require some taxonomic information that may not be provided in your data. In these cases, we will need to call taxonomic information from WoRMS using the worrms package. In addition to the required scientificName and scientificNameID fields, it is valuable to provide other information if it is available, like taxonRank, which is the lowest identifiable taxon of an occurrence, and corresponding parent taxa.\nWe will assign AphiaID from the occurrence table to a new variable myAphiaID which we will use to call the corresponding WoRMS data. Adding lapply to circumvent limits on the number of inputs, we then use wm_record(id = x), where x is myAphiaID. From the new table, we just pull the columns that we need using select.\n\nmyAphiaID &lt;- Infauna$AphiaID %&gt;% na.omit() %&gt;% unique()\n\nmyAphiaID &lt;- lapply(myAphiaID, function(x) wm_record(id = x)) %&gt;% \n  data.table::rbindlist()\n\nuniqueAphiaSelectColumns &lt;- select(.data = myAphiaID,\nscientificname, rank, kingdom, phylum, class, order, family, genus, lsid, AphiaID\n) %&gt;%\n  rename(\n    scientficName = scientificname,\n    taxonRank = rank,\n    scientificNameID = lsid\n  )\n\n#Outputting Data\nOnce our data has been cleaned up, mapped, and is ready to be uploaded to the IPT, we will have to output the data as a csv file. To do this, first use the select function to choose what columns to include in the final table. For example, the event table, it will look something like this:\n\nInfauna_Event &lt;- bind_rows(Infauna_StationCore, Infauna_Sample) %&gt;% \n  select(\n    eventID,\n    parentEventID,\n    eventDate,\n    locationID,\n    decimalLatitude,\n    decimalLongitude,\n    higherGeography,\n    locality,\n    geodeticDatum,\n    minimumDepthInMeters,\n    maximumDepthInMeters,\n    samplingProtocol,\n    locationRemarks,\n    minimumDistanceAboveSurfaceInMeters,\n    maximumDistancesAboveSurfaceInMeters,\n    materialEntityID\n  ) %&gt;% \n  distinct()\n# since there were multiple occurrences listed for events in the original dataset, here, we used distinct to make sure only unique events are included\n\nThen to export as a csv file, we will use write.csv. In in this case, we named the new file gomx_sediment_macrofauna_event_(date of export).csv.\n\nInfauna_Event %&gt;% \n  write.csv(paste0(\"gomx_sediment_macrofauna_event_\", Sys.Date(), \".csv\"))\n\n#make sure you don't pipe write.csv from the previous chunk\n\nNow we have a file we can upload to the IPT.",
    "crumbs": [
      "Wrangling the Data"
    ]
  },
  {
    "objectID": "data_modeling_page.html",
    "href": "data_modeling_page.html",
    "title": "Getting a Feel for the Data",
    "section": "",
    "text": "Starting to understand the data we are working with may not require a computer. Simple diagrams and maps can be very useful in this stage to understand the relationships and information provided in a dataset. To get started, it’s valuable to model the data and begin the process of mapping terms to DwC vocabulary.\n\nModeling the Data\nModeling the data and the relationships may be essential in understanding more complex datasets or those with multiple, related tables. This can be done with simple hand drawn diagrams or computer-generated diagrams, like mermaid diagrams.\n\n\n\nHand-drawn data visualization of relationships\n\n\n\n\n\nAnother hand-drawn representation of the relationships within and between events and occurrences\n\n\n\n\n\nMermaid diagram visualization of the same data\n\n\n\n\nMapping to Darwin Core\nOnce we have a better grasp on the relationships in our data and what cores or extensions we might use, deciding what DwC terms to use becomes easier. Going through each column, we will need to determine what DwC terms are most appropriate and if term relationships are 1:1, 1:many, many:1, or 1:0, which will be valuable in wrangling the data.\nWhile a computer is not required here, it may be useful to have access to DwC vocabulary pages to reference in this process.",
    "crumbs": [
      "Getting a Feel for the Data"
    ]
  },
  {
    "objectID": "purpose_page.html",
    "href": "purpose_page.html",
    "title": "Why Are we Doing This?",
    "section": "",
    "text": "This notebook aims to provide a general workflow for mobilizing the Demopoulos Lab’s data to OBIS to use in the future for similar tasks. Using an existing dataset as an example, we will walk through some brief contextual information; understanding, wrangling, and reconfiguring the data; adding metadata; and the publishing process.",
    "crumbs": [
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#introduction",
    "href": "purpose_page.html#introduction",
    "title": "Why Are we Doing This?",
    "section": "",
    "text": "This notebook aims to provide a general workflow for mobilizing the Demopoulos Lab’s data to OBIS to use in the future for similar tasks. Using an existing dataset as an example, we will walk through some brief contextual information; understanding, wrangling, and reconfiguring the data; adding metadata; and the publishing process.",
    "crumbs": [
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#open-science-and-fair-data",
    "href": "purpose_page.html#open-science-and-fair-data",
    "title": "Why Are we Doing This?",
    "section": "Open Science and FAIR Data",
    "text": "Open Science and FAIR Data\nOpen science is the practice of making data, resources, results, and publications as available as possible, while still respecting diverse cultures, security, and privacy.\nFAIR data is data that is findable, accessible, interoperable, and reusable. While the two often go hand-in-hand, FAIRness doesn’t require openness. However, by practicing the principles of open and FAIR science, we promote transparency in the scientific process. This transparency encourages collaboration, more efficient scientific advancements, and policy changes, while also increasing the impact of research.",
    "crumbs": [
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#obis-and-gbif",
    "href": "purpose_page.html#obis-and-gbif",
    "title": "Why Are we Doing This?",
    "section": "OBIS and GBIF",
    "text": "OBIS and GBIF\nThe Ocean Biodiversity Information System (OBIS) and Global Biodiversity Information Facility (GBIF) are international networks that mobilize data to provide free, open access to biodiversity data. They promote global scientific collaboration and open science as pillars of their missions, which is facilitated by the use of data standards and FAIR principles.",
    "crumbs": [
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#what-is-darwin-core",
    "href": "purpose_page.html#what-is-darwin-core",
    "title": "Why Are we Doing This?",
    "section": "What is Darwin Core?",
    "text": "What is Darwin Core?\nDarwin Core (DwC) is a set of data standards from Biodiversity Information Standards (TDWG). It includes standardized vocabulary and file format for Darwin Core Archives, which contains an event or occurrence core file, extension files, and metadata files.\nDwC is used by both OBIS and GBIF to help facilitate the sharing of open biological data in accordance with the FAIR principles.",
    "crumbs": [
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#what-is-eml",
    "href": "purpose_page.html#what-is-eml",
    "title": "Why Are we Doing This?",
    "section": "What is EML?",
    "text": "What is EML?\nEcological Metadata Language (EML) is a standard for recording and organizing metadata, and is required for publishing to GBIF and OBIS. It promotes transparency and accessibility of published datasets.",
    "crumbs": [
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "dataset_background_page.html",
    "href": "dataset_background_page.html",
    "title": "Dataset Background",
    "section": "",
    "text": "What data are we using in this notebook?\nWe will be using data published by USGS, titled “Sediment macrofaunal composition, sediment grain size, and taxa functional traits of multiple deep-sea coral habitats in the Gulf of Mexico, 2009-2014”, collected by Dr. Amanda Demopoulos and Dr. Jill Bourque. DOI: https://doi.org/10.5066/F7TH8KXD\nThis data describes a series of sampling events where cores are taken near one of several types coral and analyzed for sediment composition or subsampled for macrofauna.",
    "crumbs": [
      "Dataset Background"
    ]
  }
]