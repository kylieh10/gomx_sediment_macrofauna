[
  {
    "objectID": "use_stats_page.html",
    "href": "use_stats_page.html",
    "title": "Dataset Use Statistics",
    "section": "",
    "text": "To be done after publication.",
    "crumbs": [
      "Publication and Reuse",
      "Dataset Use Statistics"
    ]
  },
  {
    "objectID": "wrangling_data_page.html#renaming-columns",
    "href": "wrangling_data_page.html#renaming-columns",
    "title": "Wrangling the Data",
    "section": "Renaming Columns",
    "text": "Renaming Columns\nSome column names may directly correlate with the definitions of some DwC terms (a 1:1 relationship), so we only have to rename them. To preserve the original data, we will create a new table from Infauna to do the manipulations. Then, we will use rename(newName = oldName) to assign new column names.\n\nInfauna_StationCore &lt;- Infauna %&gt;%\n  \n  rename(\n    locationRemarks = Location,\n    materialEntityID = CoreID,\n    locationID = Station,\n    decimalLatitude = Latitude,\n    decimalLongitude = Longitude\n  )",
    "crumbs": [
      "Working with the Data",
      "Wrangling the Data"
    ]
  },
  {
    "objectID": "wrangling_data_page.html#creating-new-columns",
    "href": "wrangling_data_page.html#creating-new-columns",
    "title": "Wrangling the Data",
    "section": "Creating New Columns",
    "text": "Creating New Columns\n\nMutating\nSome renaming might be slightly more complex, requiring manipulation of a column’s format or content to fit the DwC standard. For these tasks, we will use the mutate function.\nIn this case, the column may be in the wrong format, like DateCollected which needs to be adjusted before being assigned to eventDate. Others, like eventID or higherGeography, are concatenations of other columns (many:1 relationships) and new information, which can be combined with paste. Mutate is also used to create new columns, like geodeticDatum, which was not included in the original dataset.\n\nInfauna_StationCore &lt;- Infauna_StationCore %&gt;%\n  mutate(\n    geodeticDatum = \"WGS84\",\n    eventDate = DateCollected %&gt;% \n      as.Date(\"%m/%d/%Y\"),\n    eventID = paste(Site, eventDate %&gt;% as.character(), locationID, materialEntityID,\n                    sep = \"_\") %&gt;% stringr::str_remove_all(pattern = \"-\"),\n    minimumDepthInMeters = Depth,\n    maximumDepthInMeters = Depth,\n    locality = paste(\"BOEM Lease Block\", Site),\n    higherGeography = paste(\"Gulf of Mexico\",\n                            paste(\"BOEM Lease Block\", \n                                  Site), sep = \" | \")\n  )",
    "crumbs": [
      "Working with the Data",
      "Wrangling the Data"
    ]
  },
  {
    "objectID": "wrangling_data_page.html#reconfiguring-tables",
    "href": "wrangling_data_page.html#reconfiguring-tables",
    "title": "Wrangling the Data",
    "section": "Reconfiguring Tables",
    "text": "Reconfiguring Tables\nFor some extensions, like the extendedMeasurementOrFact extension (eMoF for the cool kids), imported data may have different configurations than required by DwC, requiring reconfiguration.\n\nPivoting\nAfter renaming and mutating column names, we may have to pivot the table from wide to long format. For extendedMeasurementOrFact tables, all columns in wide format need to be pivoted into long format.\nExample of Wide Format\n\n\nspeciessite_01site_02site_03Tilia americana341Pinus strobus523\n\n\nExample of Long Format\n\n\nspeciessitecountTilia americanasite_013Tilia americanasite_023Tilia americanasite_033Pinus strobussite_015Pinus strobussite_022Pinus strobussite_034\n\n\nThis is done using the pivot_longer function. We will specify what columns to include in the pivot with cols = c(columnsToBeIncluded). All column names included in the cols function will now be in the new column measurementType using names_to and the old columns’ values in the new column measurementValue using values_to. Other columns for identifiers will also need to be added for the eMoF table and can be found in the guides mentioned in the Data Modeling page.\nWe use pivot to wrangle the eMoF table, and the pertinent chunk of code looks something like this:\n\npivot_longer(\n    cols = c(\"Thickness (transverse) of core\", \n             \"Depth (spatial coordinate) minimum relative to bed surface in the bed\", \n             \"Depth (spatial coordinate) maximum relative to bed surface in the bed\", \n             \"Proportion by volume of particles (63-2000um) in the sediment\", \n             \"Proportion by volume of particles (0-63um) in the sediment\", \n             \"Proportion by volume of particles (&gt;2000um) in the sediment\"\n             ),\n    names_to = \"measurementType\",\n    values_to = \"measurementValue\",\n    values_drop_na = TRUE\n    )\n\nHere is the full code chunk where we first do a variety of mutates prior to the pivot and show a subset of the resulting table:\n\nSedChem &lt;- SedChem %&gt;%\n  mutate(\n    SampleID = CoreID\n  )\n\nInfauna_emof &lt;- Infauna %&gt;%\n  bind_rows(SedChem) %&gt;%\n  rename(\n    materialEntityID = SampleID\n  ) %&gt;%\n  \n  mutate(\n    eventDate = DateCollected %&gt;%\n      as.Date(\"%m/%d/%Y\"),\n    eventID = paste(Site, eventDate %&gt;% as.character(), materialEntityID, sep = \"_\") %&gt;%\n      stringr::str_remove_all(pattern = \"-\"),\n    \"Depth (spatial coordinate) maximum relative to bed surface in the bed\" =\n      str_split_i(Fraction, pattern = \"-\", i = 2) %&gt;% readr::parse_number() %&gt;% \n      as.character(),\n    \"Depth (spatial coordinate) minimum relative to bed surface in the bed\" =\n      str_split_i(Fraction, pattern = \"-\", i = 1) %&gt;% readr::parse_number() %&gt;% \n      as.character(),\n    \"Proportion by volume of particles (63-2000um) in the sediment\" = as.character(Sand),\n    \"Proportion by volume of particles (0-63um) in the sediment\" = as.character(Mud),\n    \"Proportion by volume of particles (&gt;2000um) in the sediment\" = as.character(Gravel),\n    \"Thickness (transverse) of core\" = as.character(round(CoreDiameter, digits = 2))\n  ) %&gt;%\n  \n  pivot_longer(\n    cols = c(\"Thickness (transverse) of core\", \n             \"Depth (spatial coordinate) minimum relative to bed surface in the bed\", \n             \"Depth (spatial coordinate) maximum relative to bed surface in the bed\", \n             \"Proportion by volume of particles (63-2000um) in the sediment\", \n             \"Proportion by volume of particles (0-63um) in the sediment\", \n             \"Proportion by volume of particles (&gt;2000um) in the sediment\"\n             ),\n    names_to = \"measurementType\",\n    values_to = \"measurementValue\",\n    values_drop_na = TRUE\n    ) %&gt;%\n  \n  select(\n    eventID,\n    measurementType,\n    measurementValue\n  ) %&gt;% \n  distinct() %&gt;%   \n  slice(922:931) %&gt;% \n  print()\n\n# A tibble: 7 × 3\n  eventID                      measurementType                  measurementValue\n  &lt;chr&gt;                        &lt;chr&gt;                            &lt;chr&gt;           \n1 AT357_20140504_AT26144706054 Proportion by volume of particl… 44.8851836      \n2 AT357_20140504_AT26144706054 Proportion by volume of particl… 33.79811283     \n3 MC036_20140627_NA043023      Depth (spatial coordinate) mini… 0               \n4 MC036_20140627_NA043023      Depth (spatial coordinate) maxi… 10              \n5 MC036_20140627_NA043023      Proportion by volume of particl… 2.696320665     \n6 MC036_20140627_NA043023      Proportion by volume of particl… 97.04856398     \n7 MC036_20140627_NA043023      Proportion by volume of particl… 0.255115353     \n\n\n\n\nJoins\nJoins are useful if data from multiple tables need to be included in the final table, like when we make a new table with WoRMS data, which is covered in the next section. left_join will join table x to table y by values in the specified columns, so our function will join the tables Infauna_Occurrence and uniqueAphiaSelectColumns by matching AphiaID from both.\nAfter combining these tables and mutating our final iteration of scientificNameID, we have finished our occurrence table!\n\n# in this case, TSN was provided because it was listed in the original dataset, however it is not required to have both AphiaID and TSN\nOccurrence_Ext &lt;- left_join(Infauna_Occurrence, uniqueAphiaSelectColumns, by = c(\"AphiaID\" = \"AphiaID\")) %&gt;% \n  mutate(\n    TSN = paste(\"urn:lsid:itis.gov:itis_tsn:\", TSN),\n    scientificNameID = paste(scientificNameID, TSN, sep = \", \")\n  )",
    "crumbs": [
      "Working with the Data",
      "Wrangling the Data"
    ]
  },
  {
    "objectID": "data_modeling_page.html",
    "href": "data_modeling_page.html",
    "title": "Getting a Feel for the Data",
    "section": "",
    "text": "Starting to understand the data we are working with may not require a computer. Simple diagrams and maps can be very useful in this stage to understand the relationships and information provided in a dataset. To get started, it’s valuable to model the data and begin the process of mapping terms to DwC vocabulary.\n\nModeling the Data\nModeling the data and the relationships may be essential in understanding more complex datasets or those with multiple, related tables. This can be done with simple hand drawn diagrams or computer-generated diagrams, like mermaid diagrams.\n\n\n\nHand-drawn data visualization of relationships, Image by Kylie Hollis, License CC0\n\n\n\n\n\nAnother hand-drawn representation of the relationships within and between events and occurrences, Image by Kylie Hollis, License CC0\n\n\n\n\n\nMermaid diagram visualization of the same data, Image by Steve Formel, License CC0\n\n\n\n\nMapping to Darwin Core\nOnce we have a better grasp on the relationships in our data and what DwC core and extensions we might use, choosing which DwC terms to use becomes easier. Going through each column, we will need to determine what DwC terms are most appropriate and if term relationships are 1:1, 1:many, many:1, or 1:0, which will be valuable in wrangling the data.\nLet’s try to say that more simply. In the data on ScienceBase, there are csv files with column headers. We’re going to choose Darwin Core terms to rename columns (when we can) or form new columns (e.g. concatenation of columns), when needed.\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nTerm in ScienceBase data\nTerm in Darwin Core\n\n\n\n\nDateCollected\neventDate\n\n\nLatitude\ndecimalLatitude\n\n\nLongitude\ndecimalLongitude\n\n\n\n\n\nWhile a computer is not required here, it will be useful to have access to DwC vocabulary pages to reference at this point in this process.\n\nThis quick reference guide published by TDWG should be at your side during mapping: https://dwc.tdwg.org/terms/. It is never expected that you should know all the Darwin Core terms, but repeated use will help you remember those which are required by OBIS/GBIF and which ones most often apply to your work.\nThe documentation for the DwC extension is not as friendly looking, but if you dig in, it should make sense relatively quickly. GBIF publishes these here: https://rs.gbif.org/extensions.html.",
    "crumbs": [
      "Working with the Data",
      "Getting a Feel for the Data"
    ]
  },
  {
    "objectID": "intended_user_page.html",
    "href": "intended_user_page.html",
    "title": "Intended User",
    "section": "",
    "text": "The intended user of this notebook is a person who has been tasked with publishing data that has already been published to ScienceBase via the USGS data release process. However, the basic structure and concepts should apply for any data that is being mobilized (i.e. published) to GBIF and OBIS.\n\nBasic knowledge of R is needed, but you do not need to be an expert.\nBasic knowledge of the Darwin Core, and Ecological Metadata Language (EML) standards is useful, but you do not need to be an expert.\nKnowing how to use Git, and either USGS Gitlab or Github, will help the user version control the evolution of their approach to data mobilization, but if you have never used Git before, there is a section included that points you toward some good lessons.\n\nFinally, this notebook will be most useful if it can be built on through repeated use. The code underlying this notebook can be forked and developed into a new notebook for subsequent dataset mobilizations.",
    "crumbs": [
      "Background",
      "Intended User"
    ]
  },
  {
    "objectID": "purpose_page.html",
    "href": "purpose_page.html",
    "title": "Why Are We Doing This?",
    "section": "",
    "text": "This notebook provides a general workflow for mobilizing data from the USGS lab of Amanda Demopoulos to OBIS and GBIF. Using a dataset that was previously published through ScienceBase, we walk through some brief contextual information; understanding, wrangling, and reconfiguring the data; adding metadata; and the publishing process.",
    "crumbs": [
      "Background",
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#introduction",
    "href": "purpose_page.html#introduction",
    "title": "Why Are We Doing This?",
    "section": "",
    "text": "This notebook provides a general workflow for mobilizing data from the USGS lab of Amanda Demopoulos to OBIS and GBIF. Using a dataset that was previously published through ScienceBase, we walk through some brief contextual information; understanding, wrangling, and reconfiguring the data; adding metadata; and the publishing process.",
    "crumbs": [
      "Background",
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#open-science-and-fair-data",
    "href": "purpose_page.html#open-science-and-fair-data",
    "title": "Why Are We Doing This?",
    "section": "Open Science and FAIR Data",
    "text": "Open Science and FAIR Data\n\nOpen science is the practice of making data, resources, results, and publications as available as possible, while still respecting diverse cultures, security, and privacy.\nFAIR data is data that is Findable, Accessible, Interoperable, and Reusable. While the two often go hand-in-hand, FAIR data is not necessarily open data.\n\nBy practicing the principles of open and FAIR science, we follow the direction of the U.S. government. Less pragmatically, we promote transparency in the scientific process, encourage collaboration, drive efficient advancement of science and policy, and increase the impact of research.",
    "crumbs": [
      "Background",
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#obis-and-gbif",
    "href": "purpose_page.html#obis-and-gbif",
    "title": "Why Are We Doing This?",
    "section": "OBIS and GBIF",
    "text": "OBIS and GBIF\nThe Ocean Biodiversity Information System (OBIS) and Global Biodiversity Information Facility (GBIF) are international networks that mobilize data to provide free, open access to biodiversity data. They promote global scientific collaboration and open science as pillars of their missions, which is facilitated by the use of data standards and FAIR principles. GBIF and OBIS actively collaborate, and the USGS Science Analytics and Synthesis operates the US nodes for both OBIS and GBIF. However, they are distinct endeavors with separate funding streams and governance. The easiest way to differentiate the networks is that OBIS is strictly working with marine data, while GBIF accepts all data.",
    "crumbs": [
      "Background",
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#what-is-darwin-core",
    "href": "purpose_page.html#what-is-darwin-core",
    "title": "Why Are We Doing This?",
    "section": "What is Darwin Core?",
    "text": "What is Darwin Core?\nDarwin Core (DwC) is an open data standard for sharing evidence of biological occurrence. It has been developed by the Biodiversity Information Standards (TDWG), beginning in 1998. Darwin Core includes standardized vocabulary (i.e. column names for a spreadsheet) and file formats for sharing (i.e Darwin Core Archives, see below). Its used by OBIS and GBIF to help facilitate the sharing of open biological data in accordance with the FAIR principles. You may find it useful to read the relevant sections of the OBIS manual and the GBIF IPT manual.",
    "crumbs": [
      "Background",
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#what-is-eml",
    "href": "purpose_page.html#what-is-eml",
    "title": "Why Are We Doing This?",
    "section": "What is EML?",
    "text": "What is EML?\nEcological Metadata Language (EML) is a standard for recording and organizing metadata, and is the metadata that describes the entire dataset when using Darwin Core and publishing to GBIF and OBIS. Although it does not directly align with other metadata standards, like ISO 19115 and CSDGM, you will likely find it very familiar if you have ever worked with those other metadata standards. Again, the relevant sections of the OBIS manual and the GBIF IPT manual may be useful for deeper understanding.",
    "crumbs": [
      "Background",
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "purpose_page.html#darwin-core-archive",
    "href": "purpose_page.html#darwin-core-archive",
    "title": "Why Are We Doing This?",
    "section": "Darwin Core Archive",
    "text": "Darwin Core Archive\nThe Darwin Core Archive (DwC-A) is the fundamental unit of Darwin Core. It is a zip file, comprised of csv files containing the tables describing biodiversity occurrence. These tables follow a star schema (see below). The EML is included as an xml file containing metadata that describes the entire dataset. The csv files reference each other using unique identifiers, akin to keys in a SQL database. For more information, see the OBIS manual and the GBIF IPT manual.\n\n\n\nSchematic of a Darwin Core Archive, sourced from Wieczorek J, Blissett M, Braak K & Podolskiy M (2024) The GBIF Integrated Publishing Toolkit User Manual, version 3.0. Copenhagen: GBIF Secretariat. https://ipt.gbif.org/manual/en/ipt/3.0/ CC BY 4.0",
    "crumbs": [
      "Background",
      "Why are we doing this?"
    ]
  },
  {
    "objectID": "dataset_background_page.html",
    "href": "dataset_background_page.html",
    "title": "What data are we using in this notebook?",
    "section": "",
    "text": "We will be using data published by USGS, titled “Sediment macrofaunal composition, sediment grain size, and taxa functional traits of multiple deep-sea coral habitats in the Gulf of Mexico, 2009-2014”, collected by Dr. Amanda Demopoulos and Dr. Jill Bourque. DOI: https://doi.org/10.5066/F7TH8KXD\nThis data describes a series of sampling events where cores are taken near one of several types coral and analyzed for sediment composition or subsampled for macrofauna.",
    "crumbs": [
      "Background",
      "Dataset Background"
    ]
  },
  {
    "objectID": "version_control_page.html",
    "href": "version_control_page.html",
    "title": "Version Control with Git",
    "section": "",
    "text": "We recommend that you use Git to track changes to the data as you align it to Darwin Core and ready it for publishing. The goal is to produce scripts that allow other users to start with the ScienceBase data and end with the data that is shared on GBIF/OBIS.\nGit is a version control system that underlies commercial products like Github and Gitlab. Github is a public platformed owned by Microsoft and used by many scientists. In contrast, USGS maintains a private instance of Gitlab (https://code.usgs.gov/) that is regulated by USGS and subject to USGS and DOI policy about code sharing. In addition to making sure you understand how to use Git, please make sure you understand USGS policy about how to properly use Gitlab and Github.\nLessons in the basics of Git:\n\nGithub\nGitlab",
    "crumbs": [
      "Working with the Data",
      "Version Control with Git"
    ]
  },
  {
    "objectID": "publishing_page.html",
    "href": "publishing_page.html",
    "title": "Publishing to GBIF and OBIS",
    "section": "",
    "text": "Uploading to the IPT\nTo make our data available through GBIF and OBIS, we have to upload it through GBIF’s Integrated Publishing Toolkit (IPT), which can be the most complex part of data mabilization. The IPT makes a DwC-A file, similar to how Excel makes csv files, which is how the data is shared with GBIF and OBIS. There is a video of how to use the IPT that will be available soon from the US node manager. In the meantime, familiarize yourself with the IPT manual and feel free to schedule time with the US node staff (sformel@usgs.gov) when you are ready to publish.\n\n\nCreating EML Metadata\nCreating EML metadata can feel complicated, but it doesn’t need to be. In this case we were found the simplest thing to do was to manually copy values from the ScienceBase metadata into the metadata editor that is included in the IPT. However, the EML can also be created through other tools, or created programatically and uploaded to the IPT. For guidance on creating EML metadata, reference the GBIF IPT User Manual. For any further questions, reach out to GBIF or OBIS teams for assistance.\n\n\nFinal Steps\nThere is a video of how to use the IPT that will be available soon from the US node manager. In the meantime, familiarize yourself with the IPT manual and feel free to schedule time with the US node staff (sformel@usgs.gov) when you are ready to publish.",
    "crumbs": [
      "Publication and Reuse",
      "Publishing to GBIF and OBIS"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mobilizing a Treasure Trove of Deep-Sea Biodiversity Data for Global Access and Use",
    "section": "",
    "text": "Sediment macrofaunal composition, sediment grain size, and taxa functional traits of multiple deep-sea coral habitats in the Gulf of Mexico, 2009-2014\n\n\nCollected by Dr. Amanda Demopoulos and Dr. Jill Bourque\nDOI: https://doi.org/10.5066/F7TH8KXD"
  },
  {
    "objectID": "wrangling_data_page.html",
    "href": "wrangling_data_page.html",
    "title": "Wrangling the Data",
    "section": "",
    "text": "To retrieve and use the data from ScienceBase, R packages dplyr, sbtools, stringr, and worrms will need to be loaded. After loading these packages, we will read the data in from ScienceBase.\nThe ScienceBase dataset ID can be found at the end of the ScienceBase link for the dataset.\n\n\n\n\n\n\nScienceBase identifiers\n\n\n\nIf the full link to the item on ScienceBase is https://www.sciencebase.gov/catalog/item/5a709594e4b0a9a2e9d88e4e, then the identifier is 5a709594e4b0a9a2e9d88e4e.\n\n\nUsing the ScienceBase ID, we will get information about the data files using item_list_files(sb_id = sb_id) and assign it to an object sb_filenames.\nFrom the object sb_filenames, we will pull the column url. This column contains the url needed to download the data file from ScienceBase. Rather than download a local copy of the files, we will read it directly into the memory of our computer with readr:: read_csv(file = sb_filenames$url[n]), with n being the row number of the file we are reading. Now we have dataframes to work with!\n\nlibrary(dplyr)\nlibrary(sbtools)\nlibrary(stringr)\nlibrary(worrms)\nlibrary(tidyr)\n\nsb_id &lt;- '5a709594e4b0a9a2e9d88e4e'\n\nsb_filenames &lt;- item_list_files(sb_id = sb_id)\n\nBTA &lt;-readr::read_csv(file = sb_filenames$url[1])\nInfauna &lt;- readr::read_csv(file = sb_filenames$url[2])\nSedChem &lt;- readr::read_csv(file = sb_filenames$url[3])",
    "crumbs": [
      "Working with the Data",
      "Wrangling the Data"
    ]
  }
]