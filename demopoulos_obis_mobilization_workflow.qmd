---
title: "DRAFT, UNDER DEVELOPMENT: Demopoulos Lab OBIS Mobilization Workflow"
subtitle: "An elegant guide to mobilizing data to OBIS for the discerning member of the Demopoulos lab." 
date: today
author: 
  - name: "Kylie Hollis"
    orcid: "0009-0006-4061-8077"
    email: "khollis@usgs.gov"
  - name: "Stephen Formel"
    orcid: "0000-0001-7418-1244"
    email: "sformel@usgs.gov"
format: 
  html:
    toc: true
---

# Purpose
## Open Science, OBIS, and GBIF
### Open Science and FAIR Data



### OBIS and GBIF




## What is Darwin Core?

Darwin Core (DwC) is a set of data standards from Biodiversity Information Standards (TDWG). It includes standardized vocabulary and file format for Darwin Core Archives, which contains an `event` or `occurrence` core file, extension files, and metadata files.

DwC is used by both OBIS and GBIF to help facilitate the sharing of open biological data in accordance with the FAIR principles. 

## What is EML?

Ecological Metadata Language (EML)

# Intended User

# Dataset Background
## What data are we using in this notebook?

# Getting a feel for the data
  
  Does not need to be done on a computer.
  
## Modeling the Data
## Mapping to Darwin Core

# Version Control with Git
## Gitlab vs Github

# Wrangling the Data
## Using R to get the data from ScienceBase

To retrieve and use the data from ScienceBase, R packages `dplyr`, `sbtools`, `stringr`, and `worrms` will need to be loaded. After loading these packages, we will read the data in from ScienceBase.

The ScienceBase dataset ID can be found at the end of the ScienceBase link for the dataset. If the full link to the data is "https://www.sciencebase.gov/catalog/item/5a709594e4b0a9a2e9d88e4e", then the identifier is `5a709594e4b0a9a2e9d88e4e`.

Using the ScienceBase ID, we will get information about the data files using `item_list_files(sb_id = sb_id)` and assign it to an object `sb_filenames`.

From the object `sb_filenames`, we will pull the column `url`. This column contains the url needed to download the data file from ScienceBase. Rather than download a local copy of the files, we will read it directly into the memory of our computer with `readr:: read_csv(file = sb_filenames$url[n])`, with n being the row number of the file we are reading. Now we have dataframes to work with!


```{r}
#| warning: false
#| message: false

library(dplyr)
library(sbtools)
library(stringr)
library(worrms)
library(tidyr)

sb_id <- '5a709594e4b0a9a2e9d88e4e'

sb_filenames <- item_list_files(sb_id = sb_id)

BTA <-readr::read_csv(file = sb_filenames$url[1])
Infauna <- readr::read_csv(file = sb_filenames$url[2])
SedChem <- readr::read_csv(file = sb_filenames$url[3])
```

## Using R to transform the data

After loading the data, we will have to transform the data to align with DarwinCore standard formats and terms. 

### Renaming Columns

Some column names may directly correlate with the definitions of some DwC terms, meaning we only have to rename them. To preserve the original data, we will create a new table from `Infauna` to do the manipulations. Then, we will use `rename(newName = oldName)` to assign new column names.

```{r}
Infauna_StationCore <- Infauna %>%
  
  rename(
    locationRemarks = Location,
    materialEntityID = CoreID,
    locationID = Station,
    decimalLatitude = Latitude,
    decimalLongitude = Longitude
  )
```

### Mutating Columns

Some renaming might be slightly more complex, requiring manipulation of a column's format or content to fit the DwC standard. For these tasks, we will use the `mutate` function. 

In this case, the column may be in the wrong format, like `DateCollected` which needs to be adjusted before being assigned to `eventDate`. Others, like `eventDate` or `samplingProtocol`, are concatenations of other columns, which can be combined with `paste`.


```{r}
Infauna_StationCore <- Infauna_StationCore %>%
  mutate(
    geodeticDatum = "WGS84",
    eventDate = DateCollected %>% 
      as.Date("%m/%d/%Y"),
    eventID = paste(Site, eventDate %>% as.character(), locationID, materialEntityID,
                    sep = "_") %>% stringr::str_remove_all(pattern = "-"),
    minimumDepthInMeters = Depth,
    maximumDepthInMeters = Depth,
    locality = paste("BOEM Lease Block", Site),
    higherGeography = paste("Gulf of Mexico",
                            paste("BOEM Lease Block", 
                                  Site), sep = " | "),
    samplingProtocol = paste(Gear, CoreDiameter, sep = "_")
  )
```


### Adding new metadata
### Reconfiguring Tables

For some extensions, like the `extendedMeasurementOrFact` extension, imported data may have different configurations than required by DwC, requiring reconfiguration.

#### Pivoting

After renaming and mutating column names, we may have to pivot the table from wide to long format. For `extendedMeasurementOrFact` tables, all columns in wide format need to be pivoted into long format. 

(maybe provide a visual for wide vs long?)

This is done using the `pivot_longer` function. We will specify what columns to include in the pivot with `cols = c(columns to be included)`. All column names included in the `cols` function will now be under the new column named `measurementType`, assigned with `names_to` and the old columns' values under the new column `measurementValue`, assigned with `values_to`.

We use pivot to wrangle the eMoF table, and the pertinent chunk of code looks something like this:
```{r}
#| eval: false
pivot_longer(
    cols = c("COREWDTH", "MINCDIST", "MAXCDIST", "ADEPZZ01",
             "PRPCL064", "PRPCL088", "proportionGravel(>2000um)"
             ),
    names_to = "measurementType",
    values_to = "measurementValue",
    values_drop_na = TRUE
    )
```

Here is the full code chunk where we first do a variety of mutates prior to the pivot:
```{r}
SedChem <- SedChem %>%
  mutate(
    SampleID = CoreID
  )

Infauna_emof <- Infauna %>%
  bind_rows(SedChem) %>%

  rename(
    materialEntityID = SampleID,
    locationID = Station
  ) %>%

  mutate(
    eventDate = DateCollected %>%
      as.Date("%m/%d/%Y"),
    eventID = paste(Site, eventDate %>% as.character(), materialEntityID, sep = "_") %>%
      stringr::str_remove_all(pattern = "-"),
    MAXCDIST = str_split_i(Fraction, pattern = "-", i = 2) %>% readr::parse_number() %>%
      as.character(),
    MINCDIST = str_split_i(Fraction, pattern = "-", i = 1) %>% readr::parse_number() %>%
      as.character(),
    #proportion sand (63-2000um)
    "PRPCL064" = as.character(Sand),
   #proportion mud (<63um)
    "PRPCL088" = as.character(Mud),
    "proportionGravel(>2000um)" = as.character(Gravel),
    #depth below surface of water
    ADEPZZ01 = as.character(Depth),
    COREWDTH = as.character(CoreDiameter)
  ) %>%
pivot_longer(
    cols = c("COREWDTH", "MINCDIST", "MAXCDIST", "ADEPZZ01",
             "PRPCL064", "PRPCL088", "proportionGravel(>2000um)"
             ),
    names_to = "measurementType",
    values_to = "measurementValue",
    values_drop_na = TRUE
    )
```

#### Occurrences and WoRMS

#### Joins

Joins are useful if data from multiple tables need to be included in the final table. `left_join` will join table `x` to table `y` by values in specified columns. 

```{r}
#| eval: false
Occurrence_Ext <- left_join(Infauna_Occurrence, uniqueAphiaSelectColumns, by = c("AphiaID" = "AphiaID"))

```


### Writing Data

# Uploading to the IPT
# Creating EML Metadata
# Publishing to OBIS and GBIF
# Understanding Dataset Use Statistics